{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:26.254969Z",
     "start_time": "2018-11-04T05:19:25.883714Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:28.010974Z",
     "start_time": "2018-11-04T05:19:26.257380Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from analysis import rocstories as rocstories_analysis\n",
    "from datasets import rocstories\n",
    "from model_pytorch import DoubleHeadModel, load_openai_pretrained_model\n",
    "from opt import OpenAIAdam\n",
    "from text_utils import TextEncoder\n",
    "from utils import (encode_dataset, iter_data,\n",
    "                   ResultLogger, make_path)\n",
    "from loss import MultipleChoiceLossCompute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:28.072871Z",
     "start_time": "2018-11-04T05:19:28.014007Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transform_roc(X1, X2, X3):\n",
    "    n_batch = len(X1)\n",
    "    xmb = np.zeros((n_batch, 2, n_ctx, 2), dtype=np.int32)\n",
    "    mmb = np.zeros((n_batch, 2, n_ctx), dtype=np.float32)\n",
    "    start = encoder['_start_']\n",
    "    delimiter = encoder['_delimiter_']\n",
    "    for i, (x1, x2, x3), in enumerate(zip(X1, X2, X3)):\n",
    "        x12 = [start] + x1[:max_len] + [delimiter] + x2[:max_len] + [clf_token]\n",
    "        x13 = [start] + x1[:max_len] + [delimiter] + x3[:max_len] + [clf_token]\n",
    "        l12 = len(x12)\n",
    "        l13 = len(x13)\n",
    "        xmb[i, 0, :l12, 0] = x12\n",
    "        xmb[i, 1, :l13, 0] = x13\n",
    "        mmb[i, 0, :l12] = 1\n",
    "        mmb[i, 1, :l13] = 1\n",
    "    # Position information that is added to the input embeddings in the TransformerModel\n",
    "    xmb[:, :, :, 1] = np.arange(n_vocab + n_special, n_vocab + n_special + n_ctx)\n",
    "    return xmb, mmb\n",
    "\n",
    "\n",
    "def iter_apply(Xs, Ms, Ys):\n",
    "    # fns = [lambda x: np.concatenate(x, 0), lambda x: float(np.sum(x))]\n",
    "    logits = []\n",
    "    cost = 0\n",
    "    with torch.no_grad():\n",
    "        dh_model.eval()\n",
    "        for xmb, mmb, ymb in iter_data(Xs, Ms, Ys, n_batch=n_batch_train, truncate=False, verbose=True):\n",
    "            n = len(xmb)\n",
    "            XMB = torch.tensor(xmb, dtype=torch.long).to(device)\n",
    "            YMB = torch.tensor(ymb, dtype=torch.long).to(device)\n",
    "            MMB = torch.tensor(mmb).to(device)\n",
    "            _, clf_logits = dh_model(XMB)\n",
    "            clf_logits *= n\n",
    "            clf_losses = compute_loss_fct(XMB, YMB, MMB, clf_logits, only_return_losses=True)\n",
    "            clf_losses *= n\n",
    "            logits.append(clf_logits.to(\"cpu\").numpy())\n",
    "            cost += clf_losses.sum().item()\n",
    "        logits = np.concatenate(logits, 0)\n",
    "    return logits, cost\n",
    "\n",
    "\n",
    "def iter_predict(Xs, Ms):\n",
    "    logits = []\n",
    "    with torch.no_grad():\n",
    "        dh_model.eval()\n",
    "        for xmb, mmb in iter_data(Xs, Ms, n_batch=n_batch_train, truncate=False, verbose=True):\n",
    "            n = len(xmb)\n",
    "            XMB = torch.tensor(xmb, dtype=torch.long).to(device)\n",
    "            MMB = torch.tensor(mmb).to(device)\n",
    "            _, clf_logits = dh_model(XMB)\n",
    "            logits.append(clf_logits.to(\"cpu\").numpy())\n",
    "    logits = np.concatenate(logits, 0)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def log(save_dir, desc):\n",
    "    global best_score\n",
    "    print(\"Logging\")\n",
    "    tr_logits, tr_cost = iter_apply(trX[:n_valid], trM[:n_valid], trY[:n_valid])\n",
    "    va_logits, va_cost = iter_apply(vaX, vaM, vaY)\n",
    "    tr_cost = tr_cost / len(trY[:n_valid])\n",
    "    va_cost = va_cost / n_valid\n",
    "    tr_acc = accuracy_score(trY[:n_valid], np.argmax(tr_logits, 1)) * 100.\n",
    "    va_acc = accuracy_score(vaY, np.argmax(va_logits, 1)) * 100.\n",
    "    logger.log(n_epochs=n_epochs, n_updates=n_updates, tr_cost=tr_cost, va_cost=va_cost, tr_acc=tr_acc, va_acc=va_acc)\n",
    "    print('%d %d %.3f %.3f %.2f %.2f' % (n_epochs, n_updates, tr_cost, va_cost, tr_acc, va_acc))\n",
    "    if submit:\n",
    "        score = va_acc\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            path = os.path.join(save_dir, desc, 'best_params')\n",
    "            torch.save(dh_model.state_dict(), make_path(path))\n",
    "\n",
    "\n",
    "def predict(dataset, submission_dir):\n",
    "    filename = filenames[dataset]\n",
    "    pred_fn = pred_fns[dataset]\n",
    "    label_decoder = label_decoders[dataset]\n",
    "    predictions = pred_fn(iter_predict(teX, teM))\n",
    "    if label_decoder is not None:\n",
    "        predictions = [label_decoder[prediction] for prediction in predictions]\n",
    "    path = os.path.join(submission_dir, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        f.write('{}\\t{}\\n'.format('index', 'prediction'))\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            f.write('{}\\t{}\\n'.format(i, prediction))\n",
    "\n",
    "\n",
    "def run_epoch():\n",
    "    for xmb, mmb, ymb in iter_data(*shuffle(trX, trM, trYt, random_state=np.random),\n",
    "                                   n_batch=n_batch_train, truncate=True, verbose=True):\n",
    "        global n_updates\n",
    "        dh_model.train()\n",
    "        XMB = torch.tensor(xmb, dtype=torch.long).to(device)\n",
    "        YMB = torch.tensor(ymb, dtype=torch.long).to(device)\n",
    "        MMB = torch.tensor(mmb).to(device)\n",
    "        lm_logits, clf_logits = dh_model(XMB)\n",
    "        compute_loss_fct(XMB, YMB, MMB, clf_logits, lm_logits)\n",
    "        n_updates += 1\n",
    "        if n_updates in [1000, 2000, 4000, 8000, 16000, 32000] and n_epochs == 0:\n",
    "            log(save_dir, desc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:28.114577Z",
     "start_time": "2018-11-04T05:19:28.076114Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "argmax = lambda x: np.argmax(x, 1)\n",
    "\n",
    "pred_fns = {\n",
    "    'rocstories': argmax,\n",
    "}\n",
    "\n",
    "filenames = {\n",
    "    'rocstories': 'ROCStories.tsv',\n",
    "}\n",
    "\n",
    "label_decoders = {\n",
    "    'rocstories': None,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:28.170325Z",
     "start_time": "2018-11-04T05:19:28.117963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(afn='gelu', analysis=False, attn_pdrop=0.1, b1=0.9, b2=0.999, bpe_path='model/vocab_40000.bpe', clf_pdrop=0.1, data_dir='data/', dataset='data/corpus/erotic_gutenberg.csv', desc=None, e=1e-08, embd_pdrop=0.1, encoder_path='model/encoder_bpe_40000.json', l2=0.01, lm_coef=0.5, log_dir='log/', lr=6.25e-05, lr_schedule='warmup_linear', lr_warmup=0.002, max_grad_norm=1, n_batch=2, n_ctx=512, n_embd=768, n_head=12, n_iter=3, n_layer=12, n_transfer=12, n_valid=374, opt='adam', resid_pdrop=0.1, save_dir='save/', seed=42, submission_dir='submission/', submit=False, vector_l2=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--desc', type=str, help=\"Description\")\n",
    "parser.add_argument('--dataset', type=str)\n",
    "parser.add_argument('--log_dir', type=str, default='log/')\n",
    "parser.add_argument('--save_dir', type=str, default='save/')\n",
    "parser.add_argument('--data_dir', type=str, default='data/')\n",
    "parser.add_argument('--submission_dir', type=str, default='submission/')\n",
    "parser.add_argument('--submit', action='store_true')\n",
    "parser.add_argument('--analysis', action='store_true')\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "parser.add_argument('--n_iter', type=int, default=3)\n",
    "parser.add_argument('--n_batch', type=int, default=8)\n",
    "parser.add_argument('--max_grad_norm', type=int, default=1)\n",
    "parser.add_argument('--lr', type=float, default=6.25e-5)\n",
    "parser.add_argument('--lr_warmup', type=float, default=0.002)\n",
    "parser.add_argument('--n_ctx', type=int, default=512)\n",
    "parser.add_argument('--n_embd', type=int, default=768)\n",
    "parser.add_argument('--n_head', type=int, default=12)\n",
    "parser.add_argument('--n_layer', type=int, default=12)\n",
    "parser.add_argument('--embd_pdrop', type=float, default=0.1)\n",
    "parser.add_argument('--attn_pdrop', type=float, default=0.1)\n",
    "parser.add_argument('--resid_pdrop', type=float, default=0.1)\n",
    "parser.add_argument('--clf_pdrop', type=float, default=0.1)\n",
    "parser.add_argument('--l2', type=float, default=0.01)\n",
    "parser.add_argument('--vector_l2', action='store_true')\n",
    "parser.add_argument('--opt', type=str, default='adam')\n",
    "parser.add_argument('--afn', type=str, default='gelu')\n",
    "parser.add_argument('--lr_schedule', type=str, default='warmup_linear')\n",
    "parser.add_argument('--encoder_path', type=str, default='model/encoder_bpe_40000.json')\n",
    "parser.add_argument('--bpe_path', type=str, default='model/vocab_40000.bpe')\n",
    "parser.add_argument('--n_transfer', type=int, default=12)\n",
    "parser.add_argument('--lm_coef', type=float, default=0.5)\n",
    "parser.add_argument('--b1', type=float, default=0.9)\n",
    "parser.add_argument('--b2', type=float, default=0.999)\n",
    "parser.add_argument('--e', type=float, default=1e-8)\n",
    "parser.add_argument('--n_valid', type=int, default=374)\n",
    "\n",
    "\n",
    "args = parser.parse_args('''\n",
    "--dataset data/corpus/erotic_gutenberg.csv \n",
    "--n_batch 2\n",
    "'''.replace('\\n','').split(' '))\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:28.236714Z",
     "start_time": "2018-11-04T05:19:28.172849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda n_gpu 1\n"
     ]
    }
   ],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "# Constants\n",
    "submit = args.submit\n",
    "dataset = args.dataset\n",
    "n_ctx = args.n_ctx\n",
    "save_dir = args.save_dir\n",
    "desc = args.desc\n",
    "data_dir = args.data_dir\n",
    "log_dir = args.log_dir\n",
    "submission_dir = args.submission_dir\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(\"device\", device, \"n_gpu\", n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:28.279283Z",
     "start_time": "2018-11-04T05:19:28.239222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/encoder_bpe_40000.json', 'model/vocab_40000.bpe')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.encoder_path, args.bpe_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:29.029677Z",
     "start_time": "2018-11-04T05:19:28.281595Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = ResultLogger(\n",
    "    path=os.path.join(log_dir, '{}.jsonl'.format(desc)), **args.__dict__)\n",
    "\n",
    "# bpe tokenizer BYTE PAIR ENCODING https://en.wikipedia.org/wiki/Byte_pair_encoding\n",
    "# this is compression where we replace frequent pairs with unused byte codes\n",
    "text_encoder = TextEncoder(args.encoder_path, args.bpe_path)\n",
    "encoder = text_encoder.encoder\n",
    "n_vocab = len(text_encoder.encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:43.657977Z",
     "start_time": "2018-11-04T05:19:29.032614Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                       | 17/2402 [00:00<00:14, 166.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_batch_train': 2,\n",
       " 'n_train': 2402,\n",
       " 'n_updates_total': 3603,\n",
       " 'n_valid': 374,\n",
       " 'n_vocab': 40478}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Encoding dataset...\")\n",
    "((trX1, trX2, trX3, trY), (vaX1, vaX2, vaX3, vaY),\n",
    " (teX1, teX2, teX3)) = encode_dataset(\n",
    "     *rocstories(data_dir, n_valid=args.n_valid), encoder=text_encoder)\n",
    "\n",
    "encoder['_start_'] = len(encoder)\n",
    "encoder['_delimiter_'] = len(encoder)\n",
    "encoder['_classify_'] = len(encoder)\n",
    "clf_token = encoder['_classify_']\n",
    "n_special = 3\n",
    "max_len = n_ctx // 2 - 2\n",
    "\n",
    "n_ctx = min(\n",
    "    max([\n",
    "        len(x1[:max_len]) + max(len(x2[:max_len]), len(x3[:max_len]))\n",
    "        for x1, x2, x3 in zip(trX1, trX2, trX3)\n",
    "    ] + [\n",
    "        len(x1[:max_len]) + max(len(x2[:max_len]), len(x3[:max_len]))\n",
    "        for x1, x2, x3 in zip(vaX1, vaX2, vaX3)\n",
    "    ] + [\n",
    "        len(x1[:max_len]) + max(len(x2[:max_len]), len(x3[:max_len]))\n",
    "        for x1, x2, x3 in zip(teX1, teX2, teX3)\n",
    "    ]) + 3, n_ctx)\n",
    "vocab = n_vocab + n_special + n_ctx\n",
    "\n",
    "trX, trM = transform_roc(trX1, trX2, trX3)\n",
    "vaX, vaM = transform_roc(vaX1, vaX2, vaX3)\n",
    "\n",
    "if submit:\n",
    "    teX, teM = transform_roc(teX1, teX2, teX3)\n",
    "\n",
    "n_train = len(trY)\n",
    "n_valid = len(vaY)\n",
    "n_batch_train = args.n_batch * max(n_gpu, 1)\n",
    "n_updates_total = (n_train // n_batch_train) * args.n_iter\n",
    "dict(n_train=n_train, n_valid=n_valid, n_vocab=n_vocab, n_batch_train=n_batch_train, n_updates_total=n_updates_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:43.713225Z",
     "start_time": "2018-11-04T05:19:43.661514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2402, 2, 349, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, loss, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:52.414808Z",
     "start_time": "2018-11-04T05:19:43.717017Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wassname/.pyenv/versions/3.5.3/envs/jupyter3/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "dh_model = DoubleHeadModel(args, clf_token, 'multiple_choice', vocab, n_ctx)\n",
    "\n",
    "# loss, optimizer\n",
    "criterion = nn.CrossEntropyLoss(reduce=False)\n",
    "model_opt = OpenAIAdam(\n",
    "    dh_model.parameters(),\n",
    "    lr=args.lr,\n",
    "    schedule=args.lr_schedule,\n",
    "    warmup=args.lr_warmup,\n",
    "    t_total=n_updates_total,\n",
    "    b1=args.b1,\n",
    "    b2=args.b2,\n",
    "    e=args.e,\n",
    "    l2=args.l2,\n",
    "    vector_l2=args.vector_l2,\n",
    "    max_grad_norm=args.max_grad_norm)\n",
    "compute_loss_fct = MultipleChoiceLossCompute(criterion, criterion,\n",
    "                                             args.lm_coef, model_opt)\n",
    "## move up?\n",
    "# load pretrained model\n",
    "load_openai_pretrained_model(\n",
    "    dh_model.transformer, n_ctx=n_ctx, n_special=n_special)\n",
    "\n",
    "dh_model.to(device)\n",
    "dh_model = nn.DataParallel(dh_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:52.473049Z",
     "start_time": "2018-11-04T05:19:52.419773Z"
    }
   },
   "outputs": [],
   "source": [
    "n_updates = 0\n",
    "n_epochs = 0\n",
    "if dataset != 'stsb':\n",
    "    trYt = trY\n",
    "\n",
    "# save params\n",
    "if submit:\n",
    "    path = os.path.join(save_dir, desc, 'best_params')\n",
    "    torch.save(dh_model.state_dict(), make_path(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-04T05:19:25.570Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/1201 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▉                              | 297/1201 [02:19<07:12,  2.09it/s]"
     ]
    }
   ],
   "source": [
    "# run\n",
    "best_score = 0\n",
    "for i in range(args.n_iter):\n",
    "    print(\"running epoch\", i)\n",
    "    run_epoch()\n",
    "    n_epochs += 1\n",
    "    log(save_dir, desc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-04T05:19:25.576Z"
    }
   },
   "outputs": [],
   "source": [
    "if submit:\n",
    "    path = os.path.join(save_dir, desc, 'best_params')\n",
    "    dh_model.load_state_dict(torch.load(path))\n",
    "    predict(dataset, args.submission_dir)\n",
    "    if args.analysis:\n",
    "        rocstories_analysis(\n",
    "            data_dir, os.path.join(args.submission_dir, 'ROCStories.tsv'),\n",
    "            os.path.join(log_dir, 'rocstories.jsonl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "171px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "553px",
    "left": "0px",
    "right": "1191px",
    "top": "149px",
    "width": "185px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
