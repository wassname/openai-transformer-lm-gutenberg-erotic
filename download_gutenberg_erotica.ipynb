{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T02:29:54.022099Z",
     "start_time": "2018-11-04T02:29:54.017857Z"
    }
   },
   "source": [
    "# Fetch guternberg books from a category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1, get book ids\n",
    "\n",
    "- go to http://m.gutenberg.org/ebooks/search.mobile/?query=Erotic+%21+bsxErotic&sort_order=downloads\n",
    "\n",
    "- scroll to the bottom and click \"show more\" a few times\n",
    "- enter the javascript below in the browsers js console\n",
    "- it should have copied the ids to your clipboard, you can paste it into \"ids\" below\n",
    "\n",
    "\n",
    "```js\n",
    "// to get all book ids shown on page, paste this javascript into js console in browser when on the page above\n",
    "a_elems = document.getElementsByClassName(\"table link\")\n",
    "hrefs = Array.from(a_elems)\n",
    "  .map(e=>e.href) // get link\n",
    "  .filter(e=>e) // remove empty links\n",
    "ids = hrefs.map(e=>/(\\d+)\\.mobile/.exec(e)) // regular expression match\n",
    "  .filter(e=>e) // remove ones not found\n",
    "  .map(e=>e[1]) // get just id\n",
    "copy(ids) // copy to clipboard\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:17:20.517091Z",
     "start_time": "2018-11-04T05:17:20.141438Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "dest_dir = 'data/corpus/erotic_gutenberg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:17:20.567180Z",
     "start_time": "2018-11-04T05:17:20.519956Z"
    }
   },
   "outputs": [],
   "source": [
    "# urls to download text inputs\n",
    "ids = [\n",
    "  \"30254\",\n",
    "  \"30360\",\n",
    "  \"28520\",\n",
    "  \"25305\",\n",
    "  \"14005\",\n",
    "  \"28522\",\n",
    "  \"31284\",\n",
    "  \"28521\",\n",
    "  \"29827\",\n",
    "  \"52059\",\n",
    "  \"14323\",\n",
    "  \"13610\",\n",
    "  \"57284\",\n",
    "  \"13972\",\n",
    "  \"52205\",\n",
    "  \"54672\",\n",
    "  \"13614\",\n",
    "  \"28718\",\n",
    "  \"44877\",\n",
    "  \"26804\",\n",
    "  \"45150\",\n",
    "  \"37491\",\n",
    "  \"43438\",\n",
    "  \"48943\",\n",
    "  \"53807\",\n",
    "  \"26456\",\n",
    "  \"26808\",\n",
    "  \"13971\",\n",
    "  \"42406\",\n",
    "  \"43823\",\n",
    "  \"39220\",\n",
    "  \"56779\",\n",
    "  \"26809\",\n",
    "  \"18610\",\n",
    "  \"44181\",\n",
    "  \"42212\",\n",
    "  \"26806\",\n",
    "  \"42586\",\n",
    "  \"47892\",\n",
    "  \"43822\",\n",
    "  \"49855\",\n",
    "  \"26562\",\n",
    "  \"26739\",\n",
    "  \"26807\",\n",
    "  \"20568\",\n",
    "  \"40877\",\n",
    "  \"54419\",\n",
    "  \"53944\",\n",
    "  \"40557\",\n",
    "  \"29049\",\n",
    "  \"25543\",\n",
    "  \"40902\",\n",
    "  \"41301\",\n",
    "  \"56491\",\n",
    "  \"28789\",\n",
    "  \"40496\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:17:09.263186Z",
     "start_time": "2018-11-04T05:17:09.246892Z"
    }
   },
   "outputs": [],
   "source": [
    "# from https://github.com/motoom/gutenberg-ebook-scraping/blob/master/gutenberg.py\n",
    "\n",
    "# Repetitive stuff I don't want to read a 1000 times on my eBook reader.\n",
    "remove = [\"Produced by\",\"End of the Project Gutenberg\",\"End of Project Gutenberg\"]\n",
    "\n",
    "def beautify(text):\n",
    "    ''' Reads a raw Project Gutenberg etext, reformat paragraphs,\n",
    "    and removes fluff.  Determines the title of the book'''\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    collect = False\n",
    "    lookforsubtitle = False\n",
    "    outlines = []\n",
    "    startseen = endseen = False\n",
    "    title=\"\"\n",
    "    author=\"\"\n",
    "    language=\"\"\n",
    "    extra=[]\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Author: \"):\n",
    "            author = line[8:]\n",
    "        if line.startswith(\"Language: \"):\n",
    "            language = line[10:]\n",
    "        if line.startswith(\"Title: \"):\n",
    "            title = line[7:]\n",
    "            lookforsubtitle = True\n",
    "            continue\n",
    "        if lookforsubtitle:\n",
    "            if not line.strip():\n",
    "                lookforsubtitle = False\n",
    "            else:\n",
    "                subtitle = line.strip()\n",
    "                subtitle = subtitle.strip(\".\")\n",
    "                title += \", \" + subtitle\n",
    "        if (\"*** START\" in line) or (\"***START\" in line) or (line.startswith(\"*END THE SMALL PRINT!\")):\n",
    "            collect = startseen = True\n",
    "            paragraph = \"\"\n",
    "            extra.append(line)\n",
    "            continue\n",
    "        if (\"*** END\" in line) or (\"***END\" in line):\n",
    "            endseen = True\n",
    "            extra.append(line)\n",
    "            break\n",
    "        if not collect:\n",
    "            extra.append(line)\n",
    "            continue\n",
    "        if not line:\n",
    "            paragraph = paragraph.strip()\n",
    "            for term in remove:\n",
    "                if paragraph.startswith(term):\n",
    "                    extra.append(line)\n",
    "                    paragraph = \"\"\n",
    "                    break\n",
    "            if paragraph:\n",
    "                outlines.append(paragraph)\n",
    "                outlines.append(\"\")\n",
    "            paragraph = \"\"\n",
    "        else:\n",
    "            paragraph += \" \" + line\n",
    "\n",
    "    # Report on anomalous situations, but don't make it a showstopper.\n",
    "    if not title:\n",
    "#         print (ofn)\n",
    "        print (\"    Problem: No title found\\n\")\n",
    "    if not startseen:\n",
    "#         print (ofn)\n",
    "        print (\"    Problem: No '*** START' seen\\n\")\n",
    "    if not endseen:\n",
    "#         print (ofn)\n",
    "        print (\"    Problem: No '*** END' seen\\n\")\n",
    "        \n",
    "    return dict(\n",
    "        content='\\n'.join(outlines),\n",
    "        title=title,\n",
    "        author=author,\n",
    "        language=language,\n",
    "        extra=extra\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T02:41:00.594210Z",
     "start_time": "2018-11-04T02:37:49.143793Z"
    }
   },
   "outputs": [],
   "source": [
    "for bid in tqdm(ids):\n",
    "    \n",
    "    # first download index\n",
    "    index_url = \"http://www.gutenberg.org/files/{bid:}\".format(bid=bid)\n",
    "    r = requests.get(index_url)\n",
    "    r.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(r.content, \"html5lib\")\n",
    "    hrefs = [e.attrs['href'] for e in soup.findAll('a')]\n",
    "    links = [h for h in hrefs if h.endswith('.txt')]\n",
    "    \n",
    "    # download text\n",
    "    for link in links:\n",
    "        txt_url = index_url + '/' + link\n",
    "        outfile = os.path.join(dest_dir, link.replace('.txt', '.json'))\n",
    "        if not os.path.isfile(outfile):\n",
    "            r = requests.get(txt_url)\n",
    "            r.raise_for_status()\n",
    "            info = beautify(r.text)\n",
    "            if (info['language'] == 'English') and len(info['language']):\n",
    "                # TODO some are empty, check before saving\n",
    "                json.dump(info, open(outfile, 'w'))\n",
    "        \n",
    "    time.sleep(0.5) # avoid ddos/ban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T02:37:32.490814Z",
     "start_time": "2018-11-04T02:37:32.485221Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. turn into csv, like rocstories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:03.832336Z",
     "start_time": "2018-11-04T05:18:59.180170Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "dest_dir = 'data/corpus/erotic_gutenberg'\n",
    "max_len = 400\n",
    "num_sent = 6\n",
    "data=[]\n",
    "for infile in os.listdir(dest_dir):\n",
    "    path = os.path.join(dest_dir, infile)\n",
    "    info = json.load(open(path))\n",
    "    paragraphs = info['content'].split('\\n\\n')\n",
    "    for paragraph in paragraphs:\n",
    "#         sentances = [p for p in paragraph.strip().split('. ')]\n",
    "        sentances = nltk.sent_tokenize(paragraph)\n",
    "        if len(sentances)>num_sent:\n",
    "            for i in range(len(sentances)//num_sent):\n",
    "                data.append(dict(\n",
    "                    storyid=uuid.uuid4().hex,\n",
    "                    sentence1=sentances[i*5+0][:max_len],\n",
    "                    sentence2=sentances[i*5+1][:max_len],\n",
    "                    sentence3=sentances[i*5+2][:max_len],\n",
    "                    sentence4=sentances[i*5+3][:max_len],\n",
    "                    sentence5=sentances[i*5+4][:max_len],\n",
    "                    AnswerRightEnding=1\n",
    "                ))\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['storyid', 'sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'AnswerRightEnding']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:03.839834Z",
     "start_time": "2018-11-04T05:19:03.835656Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Test: concat small sentances\n",
    "# # And split large ones\n",
    "# sent = []\n",
    "# for s in sentances:\n",
    "#     if len(s)>10:\n",
    "#         sent.append(s)\n",
    "#     else:\n",
    "#         sent[-1]+=' '+s\n",
    "# sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:09.318413Z",
     "start_time": "2018-11-04T05:19:08.980242Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df['sentence1'].str.len().plot.hist(bins=55)\n",
    "df['sentence1'].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:14.924306Z",
     "start_time": "2018-11-04T05:19:14.917985Z"
    }
   },
   "outputs": [],
   "source": [
    "val_idx = int(len(df)*0.7)\n",
    "df_train = df[:val_idx]\n",
    "df_val = df[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:15.503886Z",
     "start_time": "2018-11-04T05:19:15.384466Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.to_csv('data/erotic_gutenberg_TRAIN.csv', index=False)\n",
    "df_val.to_csv('data/erotic_gutenberg_VAL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T05:19:16.452211Z",
     "start_time": "2018-11-04T05:19:16.281536Z"
    }
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# def _rocstories(path):\n",
    "#     with open(path, encoding='utf_8') as f:\n",
    "#         f = csv.reader(f)\n",
    "#         st = []\n",
    "#         ct1 = []\n",
    "#         ct2 = []\n",
    "#         y = []\n",
    "#         for i, line in enumerate(tqdm(list(f), ncols=80, leave=False)):\n",
    "#             if i > 0:\n",
    "#                 s = ' '.join(line[1:5])\n",
    "#                 c1 = line[5]\n",
    "#                 c2 = line[6]\n",
    "#                 st.append(s)\n",
    "#                 ct1.append(c1)\n",
    "#                 ct2.append(c2)\n",
    "#                 y.append(int(line[-1])-1)\n",
    "#         return st, ct1, ct2, y\n",
    "    \n",
    "# _rocstories('data/erotic_gutenberg_TRAIN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "91px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "553px",
    "left": "0px",
    "right": "1064px",
    "top": "149px",
    "width": "312px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
